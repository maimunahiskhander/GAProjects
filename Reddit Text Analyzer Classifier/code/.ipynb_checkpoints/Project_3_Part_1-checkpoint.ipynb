{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fd08c2-d5ee-408f-b88c-c38e82f4a65d",
   "metadata": {},
   "source": [
    "# Project 3: Reddit Classifier (Part 1)\n",
    "<div style=\"width: 1200px; height: 5px; background-color: white;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61255d0e-381e-4b24-821c-861e9d461f2a",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Stakeholders](#Stakeholders)\n",
    "- [Research](#Research)\n",
    "- [Scraping Reddits](#Scraping-Reddits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1264af9-c6b1-40bd-9797-eaf151aabb7e",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f3578-b7c1-4c0e-805a-f018e3aca6c5",
   "metadata": {},
   "source": [
    "We are a team of data science consultants hired by the Netflix marketing team.\n",
    "\n",
    "The Netflix marketing team seeks a model to analyze and ensure their marketing campaign aligns with keywords and themes closely associated with Disney+. \n",
    "\n",
    "The objective is to achieve a minimum of 90% F1 score, aiming to strategically redirect online search traffic towards Netflix.\n",
    "Netflix wants a marketing campaign that uses keywords and themes associated with Disney+ to attract more online search traffic to their platform rather than Disney+.\n",
    "\n",
    "This strategic maneuver will help Netflix solidify its position in the market and attract more viewers to their platform.\n",
    "\n",
    "We're tasked with building a model to assess if Netflix's campaign aligns with Disney+ semantics, targeting a 90% F1 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d2037-4247-48e8-bc45-30f689eb5c00",
   "metadata": {},
   "source": [
    "### Stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd07087-d76b-434e-8494-b26b85c7a69a",
   "metadata": {},
   "source": [
    "1. Primary: Netflix Marketing Team\n",
    "2. Secondary: Netflix Content Creation Team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ee928-59e4-4cd1-8f8e-653e83889ab2",
   "metadata": {},
   "source": [
    "### Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618d168-03a5-454d-92c9-96ca67b670ea",
   "metadata": {},
   "source": [
    "1. https://fortune.com/2022/08/11/netflix-bad-year-just-got-worse-as-disney-passes-it-in-streaming-subscribers/\n",
    "2. https://www.fool.com/investing/2022/11/13/disney-overtakes-netflix-again-who-will-win-the-st/\n",
    "3. https://www.gamingbible.com/news/tv-and-film/netflix-just-lost-18-billion-in-value-949478-20230721"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b360e-32c5-4cc2-b553-37a9eb047c3e",
   "metadata": {},
   "source": [
    "Based on the research, Netflix seems to be experiencing a decrease in subscriber growth and profit decline compared to Disney Plus. Addressing these challenges and implementing strategies to regain market share and profitability will be essential for Netflix's continued success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e8e7c-0ce4-47cd-a15a-f100551a83a2",
   "metadata": {},
   "source": [
    "### Scraping Reddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d10861c-4b1d-4d90-be3e-f8d587c11093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89f6b51-4ff0-45c3-9e10-a4ce98c2ceff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a function using PRAW to scrape chosen reddit\n",
    "def scrape_reddit(name):\n",
    "    reddit = praw.Reddit(client_id=\"vcwnGhVFebt7sHcriNOV1A\",\n",
    "                         client_secret=\"Yo4-tkkKA3N1LM0gLMaKRxaiEp6wcg\",\n",
    "                         user_agent=\"Team Streaming Services\")\n",
    "\n",
    "    chosen_subreddit = reddit.subreddit(name)\n",
    "\n",
    "    count = 0  \n",
    "\n",
    "    with open(f'../datasets/{name}_reddit_submissions.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['post_id', 'title', 'selftext', 'ups', 'upvote_ratio', 'num_comments', 'author', \n",
    "                      'link_flair_text', 'awards', 'is_original_content', 'is_video', 'post_type', \n",
    "                      'domain', 'created_utc', 'pinned', 'locked', 'stickied']\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        for category in ['hot', 'new', 'rising', 'controversial', 'top']:\n",
    "            submissions = getattr(chosen_subreddit, category)(limit=None)\n",
    "            for submission in submissions:\n",
    "                writer.writerow({\n",
    "                    'post_id': submission.id,\n",
    "                    'title': submission.title,\n",
    "                    'selftext': submission.selftext,\n",
    "                    'ups': submission.ups,\n",
    "                    'upvote_ratio': submission.upvote_ratio,\n",
    "                    'num_comments': submission.num_comments,\n",
    "                    'author': str(submission.author),\n",
    "                    'link_flair_text': submission.link_flair_text,\n",
    "                    'awards': len(submission.all_awardings),\n",
    "                    'is_original_content': submission.is_original_content,\n",
    "                    'is_video': submission.is_video,\n",
    "                    'post_type': 'text' if submission.is_self else 'link',\n",
    "                    'domain': submission.domain,\n",
    "                    'created_utc': submission.created_utc,\n",
    "                    'pinned': submission.pinned,\n",
    "                    'locked': submission.locked,\n",
    "                    'stickied': submission.stickied\n",
    "                })\n",
    "\n",
    "                count += 1\n",
    "                if count >= 10000:\n",
    "                    break\n",
    "\n",
    "            if count >= 10000:\n",
    "                break\n",
    "    \n",
    "    print(f'Completed scraping for {name}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7642825-becf-4e97-9f6b-56f9f59a7c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed scraping for Netflix!\n"
     ]
    }
   ],
   "source": [
    "scrape_reddit('Netflix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b6a68d-7a8c-4b03-aa4f-d3a0c5682498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed scraping for DisneyPlus!\n"
     ]
    }
   ],
   "source": [
    "scrape_reddit('DisneyPlus')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
